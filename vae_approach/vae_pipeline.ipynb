{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9UFWopnVdZe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9UFWopnVdZe",
        "outputId": "50b7b418-7cdd-43e5-8c6b-04a34a01c5e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1nSKLSL5VLXc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nSKLSL5VLXc",
        "outputId": "ee04c40d-f7e5-4a9f-ac80-b4f4f1489ecf"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install shap\n",
        "!pip install feature_engine\n",
        "!pip install lime\n",
        "!pip install -U imbalanced-learn scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c39a0b7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "c39a0b7f",
        "outputId": "7a308d6b-b1ea-4ace-a8b0-5965f6f42a71"
      },
      "outputs": [],
      "source": [
        "# ========== Built-in ==========\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "# ========== Data Handling ==========\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# ========== Visualization ==========\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ========== Torch ==========\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ========== Scikit-learn: Preprocessing & Utilities ==========\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler,\n",
        "    OrdinalEncoder,\n",
        ")\n",
        "\n",
        "# ========== Modeling ==========\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, ExtraTreesClassifier,\n",
        "    AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\n",
        ")\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import xgboost as xgb\n",
        "\n",
        "# ========== Scikit-learn: Model Selection ==========\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, StratifiedKFold,\n",
        "    RandomizedSearchCV, cross_val_score\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, make_scorer,\n",
        "    accuracy_score, roc_auc_score, f1_score, precision_score, recall_score,\n",
        "    roc_curve, auc\n",
        ")\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "\n",
        "# ========== Imbalanced-learn ==========\n",
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# ========== Statistical Tools ==========\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "# ========== Interpretability ==========\n",
        "import shap\n",
        "from lime import lime_tabular"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DOdsvKY6Cp-M",
      "metadata": {
        "id": "DOdsvKY6Cp-M"
      },
      "outputs": [],
      "source": [
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c93be11",
      "metadata": {
        "id": "9c93be11"
      },
      "source": [
        "I am utilizing this code as guidance, https://github.com/inab-certh/Predicting-COVID-19-severity-through-interpretable-AI-analysis-of-plasma-proteomics/blob/main/Task%202.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9251da06",
      "metadata": {
        "id": "9251da06"
      },
      "source": [
        "Categorizing ages as follows:\n",
        "- ages 0 to 40\n",
        "- ages 41 to 60\n",
        "- ages 61 to 80\n",
        "- ages 81 and above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MHr3YQu4H3tR",
      "metadata": {
        "id": "MHr3YQu4H3tR"
      },
      "outputs": [],
      "source": [
        "def bin_age(data):\n",
        "  age_bins = [0, 40, 60, 80, 100]\n",
        "  age_labels = [0, 1, 2, 3]\n",
        "\n",
        "  data['age'] = pd.cut(data['age'], bins=age_bins, labels=age_labels, right=True).astype(int)\n",
        "\n",
        "  print(\"ISB Clinical:\")\n",
        "  print(data['age'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h2YsYpbyI1hN",
      "metadata": {
        "id": "h2YsYpbyI1hN"
      },
      "outputs": [],
      "source": [
        "def target_recode_scale(value):\n",
        "    if value in [1, 2, 3, '1 or 2']:\n",
        "        return 'mild'\n",
        "    elif value in [4, 5]:\n",
        "        return 'moderate'\n",
        "    elif value in [6, 7]:\n",
        "        return 'severe'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6v6Jo73zNvZ2",
      "metadata": {
        "id": "6v6Jo73zNvZ2"
      },
      "outputs": [],
      "source": [
        "def replace_unknown_with_mode(train_column, test_column):\n",
        "    mode_value = train_column.mode()[0]\n",
        "    return train_column.replace('Unknown', mode_value), test_column.replace('Unknown', mode_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SiZ7DhozoeTk",
      "metadata": {
        "id": "SiZ7DhozoeTk"
      },
      "outputs": [],
      "source": [
        "def get_data_type(data_path, file_type='excel'):\n",
        "    if 'clinical' in data_path.lower():\n",
        "        data = pd.read_excel(data_path, keep_default_na=False, na_values=['NA'])\n",
        "    else:\n",
        "        if file_type == 'csv':\n",
        "            data = pd.read_csv(data_path, index_col=0)\n",
        "        elif file_type == 'excel':\n",
        "            data = pd.read_excel(data_path)\n",
        "        elif file_type == 'txt':\n",
        "            data = pd.read_csv(data_path, sep='\\t', index_col=0)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file type\")\n",
        "    print(\"Data loaded successfully.\")\n",
        "\n",
        "    if 'clinical' in data_path.lower():\n",
        "        data_type = 'Clinical'\n",
        "    elif 'metabolomics' in data_path.lower():\n",
        "        data_type = 'Metabolomics'\n",
        "    elif 'proteomics' in data_path.lower():\n",
        "        data_type = 'Proteomics'\n",
        "    else:\n",
        "        raise ValueError(\"Unknown data type\")\n",
        "\n",
        "    print(data.columns)\n",
        "    return data, data_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x3Gk92YAqwrZ",
      "metadata": {
        "id": "x3Gk92YAqwrZ"
      },
      "outputs": [],
      "source": [
        "def preprocess_clinical(data, output_dir):\n",
        "  data.drop('ethnicity', axis=1, inplace=True)\n",
        "  data.set_index('subject_id', inplace=True)\n",
        "  relevant_clinical_columns = ['who_severity', 'sex', 'age', 'cigarette_smoking', 'kidney_disease',\n",
        "                            'chronic_hypertension', 'cancer', 'asthma', 'copd', 'coronary_artery_disease',\n",
        "                            'respiratory_support']\n",
        "\n",
        "  data = data[relevant_clinical_columns]\n",
        "  print(\"ISB Clinical columns: \", data)\n",
        "\n",
        "  data['respiratory_support'].replace({'None': 'None'}, inplace=True)\n",
        "  print(\"These are the unique respiratory support values:\", data['respiratory_support'].unique())\n",
        "  clin_rows_with_na = data.isna().any(axis=1)\n",
        "\n",
        "  print(\"Number of rows with at least one missing value:\")\n",
        "  print(\"ISB Clinical: \", clin_rows_with_na.sum())\n",
        "\n",
        "  bin_age(data)\n",
        "  data['who_severity'] = data['who_severity'].apply(target_recode_scale)\n",
        "  print(\"ISB Clinical Severity Value Counts: \", data['who_severity'].value_counts())\n",
        "\n",
        "  y = data['who_severity'].map({'mild': 0, 'moderate': 1, 'severe': 2})\n",
        "  print(\"Target:\", y.value_counts())\n",
        "\n",
        "  X = data.drop(['who_severity'], axis = 1)\n",
        "\n",
        "  if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "  joblib.dump(X, os.path.join(output_dir, 'X_clinical.pkl'))\n",
        "\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N4Xdx8KbsEuf",
      "metadata": {
        "id": "N4Xdx8KbsEuf"
      },
      "outputs": [],
      "source": [
        "def preprocess_metabolomics(data, output_dir):\n",
        "  data.set_index('subject_id', inplace=True)\n",
        "  met_rows_with_na = data.isna().any(axis=1)\n",
        "\n",
        "  print(\"Number of rows with at least one missing value:\")\n",
        "  print(\"ISB Metabolomics: \", met_rows_with_na.sum())\n",
        "  X = data.drop(['Blood Draw', 'Healthy or INCOV', 'age', 'sex', 'BMI'], axis = 1)\n",
        "\n",
        "  if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "  joblib.dump(X, os.path.join(output_dir, 'X_metabolomics.pkl'))\n",
        "\n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "td0Cbo2-saXS",
      "metadata": {
        "id": "td0Cbo2-saXS"
      },
      "outputs": [],
      "source": [
        "def preprocess_proteomics(data, output_dir):\n",
        "  data.set_index('subject_id', inplace=True)\n",
        "  pro_rows_with_na = data.isna().any(axis=1)\n",
        "\n",
        "  print(\"Number of rows with at least one missing value:\")\n",
        "  print(\"ISB Proteomics: \", pro_rows_with_na.sum())\n",
        "  X = data.drop(['Blood Draw', 'Healthy or INCOV', 'age', 'sex', 'BMI'], axis = 1)\n",
        "\n",
        "  if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "  joblib.dump(X, os.path.join(output_dir, 'X_proteomics.pkl'))\n",
        "\n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t5vHCqj1uynF",
      "metadata": {
        "id": "t5vHCqj1uynF"
      },
      "outputs": [],
      "source": [
        "def data_split(X, y, data_type):\n",
        "  X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "  print(f\"{data_type} Train Target:\", y_train.value_counts(normalize=True))\n",
        "  return X_train, X_test, X_val, y_test, y_test, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZryxcouKHZWV",
      "metadata": {
        "id": "ZryxcouKHZWV"
      },
      "outputs": [],
      "source": [
        "def save_data_splits(X_train, X_test, X_val, y_train, y_test, y_val, output_dir, X_train_resampled=None, y_train_resampled=None):\n",
        "    \"\"\"\n",
        "    Saves the train-test data splits to the specified directory using joblib.\n",
        "\n",
        "    Args:\n",
        "    - X_train (pd.DataFrame or np.ndarray): Training data features.\n",
        "    - X_test (pd.DataFrame or np.ndarray): Testing data features.\n",
        "    - y_train (pd.Series or np.ndarray): Training data labels.\n",
        "    - y_test (pd.Series or np.ndarray): Testing data labels.\n",
        "    - output_dir (str): Directory path where the data splits will be saved.\n",
        "\n",
        "    Note:\n",
        "    - If the output directory does not exist, it will be created.\n",
        "    \"\"\"\n",
        "\n",
        "    assert X_train.shape[0] == y_train.shape[0], \"Mismatch in training data and labels.\"\n",
        "    assert X_test.shape[0] == y_test.shape[0], \"Mismatch in testing data and labels.\"\n",
        "    assert X_val.shape[0] == y_val.shape[0], \"Mismatch in validation data and labels.\"\n",
        "\n",
        "    print(\"Training set size:\", X_train.shape, y_train.shape)\n",
        "    print(\"Testing set size:\", X_test.shape, y_test.shape)\n",
        "    print(\"Validation set size:\", X_val.shape, y_val.shape)\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    joblib.dump(X_train, os.path.join(output_dir, 'X_train.pkl'))\n",
        "    joblib.dump(X_test, os.path.join(output_dir, 'X_test.pkl'))\n",
        "    joblib.dump(X_val, os.path.join(output_dir, 'X_val.pkl'))\n",
        "    joblib.dump(y_train, os.path.join(output_dir, 'y_train.pkl'))\n",
        "    joblib.dump(y_test, os.path.join(output_dir, 'y_test.pkl'))\n",
        "    joblib.dump(y_val, os.path.join(output_dir, 'y_val.pkl'))\n",
        "\n",
        "    if X_train_resampled is not None and y_train_resampled is not None:\n",
        "        joblib.dump(X_train_resampled, os.path.join(output_dir, 'X_train_resampled.pkl'))\n",
        "        joblib.dump(y_train_resampled, os.path.join(output_dir, 'y_train_resampled.pkl'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q4ympeT2t9yP",
      "metadata": {
        "id": "q4ympeT2t9yP"
      },
      "outputs": [],
      "source": [
        "def fill_missing(X_train, X_test, X_val):\n",
        "  columns_to_fill = ['cancer', 'cigarette_smoking', 'asthma', 'copd', 'coronary_artery_disease']\n",
        "  for column in columns_to_fill:\n",
        "    if column in X_train.columns:\n",
        "      X_train[column], X_test[column] = replace_unknown_with_mode(X_train[column], X_test[column])\n",
        "      X_train[column], X_val[column] = replace_unknown_with_mode(X_train[column], X_val[column])\n",
        "  return X_train, X_test, X_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce9B02nyuPQb",
      "metadata": {
        "id": "ce9B02nyuPQb"
      },
      "outputs": [],
      "source": [
        "def encode_ordinal_columns(X_train, X_test, X_val):\n",
        "  cigarette_ordering = [['Never', 'Former', 'Current']]\n",
        "  respiratory_support_ordering = [['None', 'Other', 'Nasal cannula', 'High flow nasal cannula (HFNC)']]\n",
        "  encoder_cigarette = OrdinalEncoder(categories=cigarette_ordering)\n",
        "  encoder_respiratory_support = OrdinalEncoder(categories=respiratory_support_ordering)\n",
        "  columns_to_encode = ['cigarette_smoking', 'respiratory_support']\n",
        "  for column in columns_to_encode:\n",
        "      if column in X_train.columns:\n",
        "        if column == 'cigarette_smoking':\n",
        "            X_train['cigarette_smoking'] = encoder_cigarette.fit_transform(X_train[[column]])\n",
        "            X_test['cigarette_smoking'] = encoder_cigarette.transform(X_test[[column]])\n",
        "            X_val['cigarette_smoking'] = encoder_cigarette.transform(X_val[[column]])\n",
        "        elif column == 'respiratory_support':\n",
        "            X_train['respiratory_support'] = encoder_respiratory_support.fit_transform(X_train[[column]])\n",
        "            X_test['respiratory_support'] = encoder_respiratory_support.transform(X_test[[column]])\n",
        "            X_val['respiratory_support'] = encoder_respiratory_support.transform(X_val[[column]])\n",
        "  return X_train, X_test, X_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hP8TfDekuhWH",
      "metadata": {
        "id": "hP8TfDekuhWH"
      },
      "outputs": [],
      "source": [
        "def encode_binary_columns(X_train, X_test, X_val):\n",
        "  binary_columns = ['sex', 'kidney_disease', 'chronic_hypertension', 'cancer', 'asthma', 'copd', 'coronary_artery_disease']\n",
        "  binary_mapping = {'Male': 1, 'Yes': 1, 'Female': 0, 'No': 0}\n",
        "\n",
        "  for column in binary_columns:\n",
        "    if column in X_train.columns:\n",
        "      X_train[column] = X_train[column].map(binary_mapping)\n",
        "      X_val[column] = X_val[column].map(binary_mapping)\n",
        "      X_test[column] = X_test[column].map(binary_mapping)\n",
        "  return X_train, X_test, X_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-AYonICfT5Lx",
      "metadata": {
        "id": "-AYonICfT5Lx"
      },
      "outputs": [],
      "source": [
        "def corr_matrix(X_train, output_dir):\n",
        "    corr_matrix = X_train.corr().abs()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_matrix, cmap='coolwarm', center=0, linewidths=1, annot=False, fmt=\".2f\")\n",
        "\n",
        "    output_file = os.path.join(output_dir, 'correlation_matrix.png')\n",
        "\n",
        "    plt.savefig(output_file)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tEoP67iGUvH5",
      "metadata": {
        "id": "tEoP67iGUvH5"
      },
      "outputs": [],
      "source": [
        "def multicollinearity(X_test, output_dir):\n",
        "    df_const = add_constant(X_test)\n",
        "\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"Variable\"] = df_const.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(df_const.values, i) for i in range(df_const.shape[1])]\n",
        "\n",
        "    print(vif_data)\n",
        "\n",
        "    output_path = os.path.join(output_dir, \"vif_data.csv\")\n",
        "\n",
        "    vif_data.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SDFEbPM9jh9O",
      "metadata": {
        "id": "SDFEbPM9jh9O"
      },
      "outputs": [],
      "source": [
        "proteomics_scaling_selection = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('selector', RFE(estimator=LogisticRegression(), n_features_to_select=100))\n",
        "])\n",
        "\n",
        "metabolomics_scaling_selection = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('selector', RFE(estimator=LogisticRegression(), n_features_to_select=100))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ifBnIkrfk7vP",
      "metadata": {
        "id": "ifBnIkrfk7vP"
      },
      "outputs": [],
      "source": [
        "def omics_pipeline(X_train, X_val, X_test, y_train, data_type):\n",
        "    if data_type == 'Clinical':\n",
        "        print(\"Clinical data received. Skipping preprocessing.\")\n",
        "        return X_train, X_val, X_test\n",
        "\n",
        "    elif data_type == 'Metabolomics':\n",
        "        scaling_selection = metabolomics_scaling_selection\n",
        "    elif data_type == 'Proteomics':\n",
        "        scaling_selection = proteomics_scaling_selection\n",
        "    else:\n",
        "        raise ValueError(f\"Data type {data_type} is not supported for this preprocessing.\")\n",
        "\n",
        "    # fit and transform the training data\n",
        "    X_train_preprocessed = scaling_selection.fit_transform(X_train, y_train)\n",
        "    selected_features = X_train.columns[scaling_selection.named_steps['selector'].support_]\n",
        "\n",
        "    # transform the validation and test data\n",
        "    X_val_preprocessed = scaling_selection.transform(X_val)\n",
        "    X_test_preprocessed = scaling_selection.transform(X_test)\n",
        "\n",
        "    # convert arrays back to DataFrame with selected feature names and original index\n",
        "    X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=selected_features, index=X_train.index)\n",
        "    X_val_preprocessed = pd.DataFrame(X_val_preprocessed, columns=selected_features, index=X_val.index)\n",
        "    X_test_preprocessed = pd.DataFrame(X_test_preprocessed, columns=selected_features, index=X_test.index)\n",
        "\n",
        "    return X_train_preprocessed, X_val_preprocessed, X_test_preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3Qrd6TChDfbR",
      "metadata": {
        "id": "3Qrd6TChDfbR"
      },
      "outputs": [],
      "source": [
        "class SimpleVAE(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(SimpleVAE, self).__init__()\n",
        "        # encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, latent_dim * 2) \n",
        "        )\n",
        "        # decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, input_dim) \n",
        "        )\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        mean, log_var = encoded.chunk(2, dim=1)\n",
        "        return mean, log_var\n",
        "\n",
        "    def reparameterize(self, mean, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mean + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mean, log_var)\n",
        "        return self.decode(z), mean, log_var\n",
        "\n",
        "    def get_latent(self, x):\n",
        "        mean, log_var = self.encode(x)\n",
        "        return self.reparameterize(mean, log_var)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TKU5kK1Cvrkx",
      "metadata": {
        "id": "TKU5kK1Cvrkx"
      },
      "outputs": [],
      "source": [
        "def vae_process(X_train, X_val, X_test, device, data_type, epochs=50):\n",
        "    input_dim = X_train.shape[1]\n",
        "    print(\"This is input dim: \", input_dim)\n",
        "    latent_dim = input_dim // 3\n",
        "\n",
        "    vae_model = SimpleVAE(input_dim, latent_dim).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(vae_model.parameters(), lr=1e-3)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
        "    X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
        "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae_model.train()\n",
        "        optimizer.zero_grad()\n",
        "        reconstructed, mean, log_var = vae_model(X_train_tensor)\n",
        "        reconstruction_loss = criterion(reconstructed, X_train_tensor)\n",
        "        kl_divergence = -0.5 * torch.mean(1 + log_var - mean.pow(2) - log_var.exp())\n",
        "        loss = reconstruction_loss + kl_divergence\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "    model_save_path = os.path.join('vae_outputs', data_type, 'vae_model.pth')\n",
        "    torch.save(vae_model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved at {model_save_path}\")\n",
        "\n",
        "    vae_model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_train_latent = vae_model.get_latent(X_train_tensor)\n",
        "        X_val_latent = vae_model.get_latent(X_val_tensor)\n",
        "        X_test_latent = vae_model.get_latent(X_test_tensor)\n",
        "\n",
        "        X_train_reconstructed = vae_model.decode(X_train_latent).cpu().numpy()\n",
        "        X_val_reconstructed = vae_model.decode(X_val_latent).cpu().numpy()\n",
        "        X_test_reconstructed = vae_model.decode(X_test_latent).cpu().numpy()\n",
        "\n",
        "    output_dir = os.path.join('vae_outputs', data_type)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    np.save(os.path.join(output_dir, 'X_train_reconstructed.npy'), X_train_reconstructed)\n",
        "    np.save(os.path.join(output_dir, 'X_val_reconstructed.npy'), X_val_reconstructed)\n",
        "    np.save(os.path.join(output_dir, 'X_test_reconstructed.npy'), X_test_reconstructed)\n",
        "\n",
        "    return X_train_reconstructed, X_val_reconstructed, X_test_reconstructed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "145a6a7b",
      "metadata": {
        "id": "145a6a7b"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    'logistic_regression': LogisticRegression(),\n",
        "    'decision_tree': DecisionTreeClassifier(),\n",
        "    'rand_forest': RandomForestClassifier(),\n",
        "    'extra_trees': ExtraTreesClassifier(),\n",
        "    'mlp': MLPClassifier(),\n",
        "    'svm': SVC(),\n",
        "    'ada_boost': AdaBoostClassifier(),\n",
        "    'xgb': xgb.XGBClassifier(),\n",
        "    'gbm': GradientBoostingClassifier(),\n",
        "    'qda': QuadraticDiscriminantAnalysis(),\n",
        "}\n",
        "\n",
        "log_params = {\n",
        "    'logistic_regression__penalty': ['l1', 'l2'],\n",
        "    'logistic_regression__C': [0.1, 1, 10],\n",
        "    'logistic_regression__solver': ['liblinear', 'saga'],\n",
        "    'logistic_regression__max_iter': [100, 200, 300, 400, 500, 600, 700, 800, 900],\n",
        "    'logistic_regression__class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "\n",
        "dt_params = {\n",
        "    'decision_tree__criterion': ['gini', 'entropy'],\n",
        "    'decision_tree__max_depth': [None,1, 5, 10, 50, 100],\n",
        "    'decision_tree__min_samples_split': [2, 5, 10],\n",
        "    'decision_tree__min_samples_leaf': [1, 2, 4],\n",
        "    'decision_tree__splitter':['best','random'],\n",
        "    'decision_tree__class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "rf_params = {\n",
        "    'rand_forest__n_estimators':[50, 100, 500, 1000, 2000, 2500],\n",
        "    'rand_forest__criterion': ['gini', 'entropy'],\n",
        "    'rand_forest__max_depth': [None, 5, 10, 20],\n",
        "    'rand_forest__min_samples_split': [2, 5, 10],\n",
        "    'rand_forest__min_samples_leaf': [1, 2, 4],\n",
        "    'rand_forest__class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "et_params = {\n",
        "    'extra_trees__n_estimators': [100, 200, 500, 1000, 2000],\n",
        "    'extra_trees__criterion': ['gini', 'entropy'],\n",
        "    'extra_trees__max_depth': [None, 5, 10],\n",
        "    'extra_trees__min_samples_split': [2, 5, 10],\n",
        "    'extra_trees__min_samples_leaf': [1, 2, 4],\n",
        "    'extra_trees__class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "mlp_params = {\n",
        "    'mlp__hidden_layer_sizes': [(100,), (100, 50), (50, 50), (50,50,50)],\n",
        "    'mlp__activation': ['relu', 'tanh', 'logistic', 'identity'],\n",
        "    'mlp__solver': ['adam', 'sgd', 'lbfgs'],\n",
        "    'mlp__learning_rate':['constant','adaptive'],\n",
        "    'mlp__alpha': [0.0001, 0.001, 0.01, 0.05],\n",
        "    'mlp__max_iter': [100, 200, 300, 400, 500, 600, 700, 800, 900]\n",
        "}\n",
        "\n",
        "svm_params = {\n",
        "    'svm__C':[0.1, 1, 2, 5, 10, 50, 100, 500],\n",
        "    'svm__kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
        "    'svm__gamma': ['scale', 'auto'],\n",
        "    'svm__probability': [True],\n",
        "    'svm__class_weight': [None, 'balanced'],\n",
        "    'svm__max_iter': [100, 200, 300, 400, 500, 600, 700, 800, 900]\n",
        "}\n",
        "\n",
        "ada_params = {\n",
        "    'ada_boost__n_estimators': [10, 20, 30, 50, 100, 200, 500, 1000],\n",
        "    'ada_boost__learning_rate': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
        "}\n",
        "\n",
        "xgb_params = {\n",
        "    'xgb__n_estimators': [100, 200, 500, 1000, 2000],\n",
        "    'xgb__learning_rate': [0.01, 0.03, 0.06, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
        "    'xgb__max_depth': [3, 5, 7, 9, 11],\n",
        "    'xgb__subsample': [0.8, 1.0],\n",
        "    'xgb__colsample_bytree': [0.8, 1.0],\n",
        "    'xgb__gamma': [0, 0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8, 25.6, 51.2, 102.4, 200],\n",
        "    'xgb__reg_alpha': [0, 0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8, 25.6, 51.2, 102.4, 200],\n",
        "    'xgb__reg_lambda': [0, 0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8, 25.6, 51.2, 102.4, 200]\n",
        "}\n",
        "\n",
        "gbm_params = {\n",
        "    'gbm__n_estimators': [50, 100, 200, 500, 1000, 2000, 2500, 3000],\n",
        "    'gbm__learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4],\n",
        "    'gbm__max_depth': [3, 5, 7],\n",
        "    'gbm__subsample': [0.8, 1.0],\n",
        "    'gbm__max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "qda_params = {\n",
        "    'qda__reg_param': [0, 0.5, 1]\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "params = {\n",
        "    'logistic_regression': log_params,\n",
        "    'decision_tree': dt_params,\n",
        "    'rand_forest': rf_params,\n",
        "    'extra_trees': et_params,\n",
        "    'mlp': mlp_params,\n",
        "    'svm': svm_params,\n",
        "    'ada_boost': ada_params,\n",
        "    'xgb': xgb_params,\n",
        "    'gbm': gbm_params,\n",
        "    'qda': qda_params,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec9d27a8",
      "metadata": {
        "id": "ec9d27a8"
      },
      "outputs": [],
      "source": [
        "def model_search(models, params, over, under, X_train, y_train, X_test, y_test, X_val, y_val):\n",
        "    print(f\"Number of models: {len(models)}\")\n",
        "    training_times = []\n",
        "    max_score = 0\n",
        "    max_model = None\n",
        "    max_model_params = None\n",
        "    estimators_modelsearch = pd.DataFrame()\n",
        "    models_est = []\n",
        "    parameters = []\n",
        "    lscore = []\n",
        "    lroc = []\n",
        "    lfscore_macro = []\n",
        "    lfscore_weighted = []\n",
        "    lprecision = []\n",
        "    lrecall = []\n",
        "    lrecall_severe = []\n",
        "    lclass_report = []\n",
        "    lcvscore = []\n",
        "    lscore_val = []\n",
        "    lroc_val = []\n",
        "    lfscore_macro_val = []\n",
        "    lfscore_weighted_val = []\n",
        "    lprecision_val = []\n",
        "    lrecall_val = []\n",
        "    lrecall_severe_val = []\n",
        "    lclass_report_val = []\n",
        "\n",
        "    scoring = {\n",
        "        'balanced_accuracy': 'balanced_accuracy',\n",
        "        'accuracy': 'accuracy',\n",
        "        'roc_auc': 'roc_auc_ovr',\n",
        "        'precision': 'precision_macro',\n",
        "        'recall_macro': make_scorer(recall_score, average='macro'),\n",
        "        'f1_macro': make_scorer(f1_score, average='macro'),\n",
        "        'f1_weighted': make_scorer(f1_score, average='weighted')\n",
        "    }\n",
        "\n",
        "    cv = StratifiedKFold(10)\n",
        "\n",
        "\n",
        "    for i, j in models.items():\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            pipeline = Pipeline([('over', over), ('under', under), (i, j)])\n",
        "\n",
        "            rs = RandomizedSearchCV(\n",
        "                estimator=pipeline,\n",
        "                param_distributions=params[i],\n",
        "                scoring=scoring,\n",
        "                refit='roc_auc',\n",
        "                cv=cv,\n",
        "                n_iter=30,\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            rs.fit(X_train, y_train)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error while fitting model {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "        y_pred = rs.predict(X_test)\n",
        "        class_report = classification_report(y_test, y_pred)\n",
        "        print(f\"Classification Report for {i}:\\n\", class_report)\n",
        "\n",
        "        y_pred_val = rs.predict(X_val)\n",
        "        class_report_val = classification_report(y_val, y_pred_val)\n",
        "        print(f\"Validation Classification Report for {i}:\\n\", class_report_val)\n",
        "\n",
        "        try:\n",
        "            y_pred_proba = rs.predict_proba(X_test)\n",
        "            lroc.append(roc_auc_score(y_test, y_pred_proba, multi_class='ovr'))\n",
        "        except AttributeError:\n",
        "            print(f\"{i} does not support probability prediction. ROC AUC score cannot be computed.\")\n",
        "            lroc.append(None)\n",
        "\n",
        "        try:\n",
        "            y_pred_proba_val = rs.predict_proba(X_val)\n",
        "            lroc_val.append(roc_auc_score(y_val, y_pred_proba_val, multi_class='ovr'))\n",
        "        except AttributeError:\n",
        "            print(f\"{i} does not support probability prediction. ROC AUC score cannot be computed for validation set.\")\n",
        "            lroc_val.append(None)\n",
        "\n",
        "\n",
        "\n",
        "        lscore.append(accuracy_score(y_test, y_pred))\n",
        "        lscore_val.append(accuracy_score(y_val, y_pred_val))\n",
        "        lcvscore.append(rs.best_score_)\n",
        "\n",
        "        try:\n",
        "            lprecision.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
        "            lprecision_val.append(precision_score(y_val, y_pred_val, average='macro', zero_division=0))\n",
        "        except UndefinedMetricWarning:\n",
        "            print(f\"Warning encountered for model {i}\")\n",
        "            print(\"Test predictions:\", y_pred)\n",
        "            print(\"Validation predictions:\", y_pred_val)\n",
        "\n",
        "\n",
        "        lrecall.append(recall_score(y_test, y_pred, average='macro'))\n",
        "        lrecall_val.append(recall_score(y_val, y_pred_val, average='macro'))\n",
        "\n",
        "        lfscore_macro.append(f1_score(y_test, y_pred, average='macro'))\n",
        "        lfscore_macro_val.append(f1_score(y_val, y_pred_val, average='macro'))\n",
        "\n",
        "        lfscore_weighted.append(f1_score(y_test, y_pred, average='weighted'))\n",
        "        lfscore_weighted_val.append(f1_score(y_val, y_pred_val, average='weighted'))\n",
        "\n",
        "        lrecall_severe.append(recall_score(y_test, y_pred, labels=[2], average='macro'))\n",
        "        lrecall_severe_val.append(recall_score(y_val, y_pred_val, labels=[2], average='macro'))\n",
        "\n",
        "        lclass_report.append(class_report)\n",
        "        lclass_report_val.append(class_report_val)\n",
        "\n",
        "        if lscore[-1] > max_score:\n",
        "            max_score = lscore[-1]\n",
        "            max_model = rs.best_estimator_\n",
        "            max_model_params = rs.best_params_\n",
        "\n",
        "        models_est.append(i)\n",
        "        parameters.append(rs.best_params_)\n",
        "\n",
        "    estimators_modelsearch['Models'] = models_est\n",
        "    estimators_modelsearch['Best parameters'] = parameters\n",
        "    estimators_modelsearch['ROC AUC'] = lroc\n",
        "    estimators_modelsearch['ROC AUC Validation'] = lroc_val\n",
        "    estimators_modelsearch['F1-score (macro)'] = lfscore_macro\n",
        "    estimators_modelsearch['F1-score (macro) Validation'] = lfscore_macro_val\n",
        "    estimators_modelsearch['F1-score (weighted)'] = lfscore_weighted\n",
        "    estimators_modelsearch['F1-score (weighted) Validation'] = lfscore_weighted_val\n",
        "    estimators_modelsearch['Precision'] = lprecision\n",
        "    estimators_modelsearch['Precision Validation'] = lprecision_val\n",
        "    estimators_modelsearch['Recall'] = lrecall\n",
        "    estimators_modelsearch['Recall Severe'] = lrecall_severe\n",
        "    estimators_modelsearch['Recall Severe Validation'] = lrecall_severe_val\n",
        "    estimators_modelsearch['Score'] = lscore\n",
        "    estimators_modelsearch['Score Validation'] = lscore_val\n",
        "    estimators_modelsearch['CV Score'] = lcvscore\n",
        "    print(len(estimators_modelsearch))\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.bar(models_est, lscore, label='Test Score')\n",
        "    plt.bar(models_est, lscore_val, label='Validation Score', alpha=0.5)\n",
        "    plt.legend()\n",
        "    plt.title(\"Performance Comparison\")\n",
        "    plt.show()\n",
        "\n",
        "    return [max_score, max_model, max_model_params], estimators_modelsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bqG60_wnwuE",
      "metadata": {
        "id": "2bqG60_wnwuE"
      },
      "outputs": [],
      "source": [
        "def plot_roc_curve(y_test, y_prob, output_dir):\n",
        "    \"\"\"\n",
        "    Plots the Receiver Operating Characteristic (ROC) curve for multi-class classification and saves the plot.\n",
        "\n",
        "    Args:\n",
        "    - y_test (pd.Series or np.ndarray): True labels for the test data.\n",
        "    - y_prob (np.ndarray): Probability estimates of the positive class for each class.\n",
        "    - output_dir (str): Directory path where the ROC curve plot will be saved.\n",
        "\n",
        "    Note:\n",
        "    - The function handles multi-class classification by plotting an ROC curve for each class.\n",
        "    - If the provided `output_dir` does not exist, it must be created before calling this function.\n",
        "    \"\"\"\n",
        "    n_classes = len(np.unique(y_test))\n",
        "    mlb = MultiLabelBinarizer(classes=list(range(n_classes)))\n",
        "    y_test_binarized = mlb.fit_transform(y_test.to_numpy().reshape(-1, 1))\n",
        "\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_prob[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    macro_roc_auc = roc_auc_score(y_test_binarized, y_prob, average='macro')\n",
        "    for i in range(n_classes):\n",
        "        print(f\"Class {i} ROC AUC: {roc_auc[i]:.2f}\")\n",
        "    print(f\"Macro-average ROC AUC: {macro_roc_auc:.2f}\")\n",
        "\n",
        "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i, color in zip(range(n_classes), colors):\n",
        "        plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve (area = {roc_auc[i]:.2f}) for class {i}')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig(os.path.join(output_dir, 'roc_curve.png'))\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OlW058Dgou89",
      "metadata": {
        "id": "OlW058Dgou89"
      },
      "outputs": [],
      "source": [
        "def main(data_paths):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    y = None\n",
        "    for data_path in data_paths:\n",
        "        data, data_type = get_data_type(data_path, file_type='excel')\n",
        "        if data_type == 'Clinical':\n",
        "            output_dir = os.path.join('covid_baseline_classifier_outputs', data_type)\n",
        "            X, y = preprocess_clinical(data, output_dir)\n",
        "        elif data_type == 'Metabolomics':\n",
        "            if y is None:\n",
        "                raise ValueError(\"Clinical data must be processed first to set 'y'.\")\n",
        "            output_dir = os.path.join('covid_baseline_classifier_outputs', data_type)\n",
        "            X = preprocess_metabolomics(data, output_dir)\n",
        "        elif data_type == 'Proteomics':\n",
        "            if y is None:\n",
        "                raise ValueError(\"Clinical data must be processed first to set 'y'.\")\n",
        "            output_dir = os.path.join('covid_baseline_classifier_outputs', data_type)\n",
        "            X = preprocess_proteomics(data, output_dir)\n",
        "\n",
        "        print(np.unique(y))\n",
        "        print(\"X shape:\", X.shape)\n",
        "        print(\"y shape:\", y.shape)\n",
        "        print(\"Size before first split:\", X.shape, y.shape)\n",
        "        X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "        print(\"Size after first split - Test set:\", X_test.shape, y_test.shape)\n",
        "\n",
        "        print(\"Size before second split:\", X_temp.shape, y_temp.shape)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "        print(\"Sizes after second split - Train and Val set:\", X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
        "\n",
        "        print(f\"{data_type} Train Target:\", y_train.value_counts(normalize=True))\n",
        "\n",
        "        X_train = X_train.copy()\n",
        "        X_test = X_test.copy()\n",
        "        X_val = X_val.copy()\n",
        "\n",
        "        X_train, X_test, X_val = fill_missing(X_train, X_test, X_val)\n",
        "        X_train, X_test, X_val = encode_ordinal_columns(X_train, X_test, X_val)\n",
        "\n",
        "        if 'age' in X_train.columns:\n",
        "            age_train = X_train.pop('age')\n",
        "            X_train['age'] = age_train\n",
        "            age_val = X_val.pop('age')\n",
        "            X_val['age'] = age_val\n",
        "            age_test = X_test.pop('age')\n",
        "            X_test['age'] = age_test\n",
        "\n",
        "        if data_type == 'Clinical':\n",
        "            X_train, X_test, X_val = encode_binary_columns(X_train, X_test, X_val)\n",
        "            corr_matrix(X_train, output_dir)\n",
        "            multicollinearity(X_test, output_dir)\n",
        "\n",
        "            print(\"Class distribution before resampling:\", np.bincount(y_train))\n",
        "            over = BorderlineSMOTE(sampling_strategy='auto')\n",
        "            under = RandomUnderSampler(sampling_strategy='majority')\n",
        "\n",
        "            pipeline = Pipeline([\n",
        "                ('over', over),\n",
        "                ('under', under)\n",
        "            ])\n",
        "\n",
        "            X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)\n",
        "\n",
        "            print(\"Class distribution after resampling:\", np.bincount(y_train_resampled))\n",
        "\n",
        "\n",
        "            results, df = model_search(\n",
        "                models=models,\n",
        "                params=params,\n",
        "                over=over,\n",
        "                under=under,\n",
        "                X_train=X_train_resampled,\n",
        "                y_train=y_train_resampled,\n",
        "                X_test=X_test,\n",
        "                y_test=y_test,\n",
        "                X_val=X_val,\n",
        "                y_val=y_val\n",
        "            )\n",
        "            save_data_splits(X_train_resampled, X_test, X_val, y_train_resampled, y_test, y_val, os.path.join('covid_baseline_classifier_outputs', data_type), X_train_resampled, y_train_resampled)\n",
        "            file_path = data_type + '_model_training_results.xlsx'\n",
        "            df.to_excel(file_path)\n",
        "\n",
        "        elif data_type == 'Metabolomics':\n",
        "            X_train_preprocessed, X_val_preprocessed, X_test_preprocessed = omics_pipeline(X_train, X_val, X_test, y_train, data_type)\n",
        "\n",
        "            X_train_reconstructed, X_val_reconstructed, X_test_reconstructed = vae_process(X_train_preprocessed, X_val_preprocessed, X_test_preprocessed, device, data_type)\n",
        "\n",
        "            print(\"Class distribution before resampling:\", np.bincount(y_train))\n",
        "            over = BorderlineSMOTE(sampling_strategy='auto')\n",
        "            under = RandomUnderSampler(sampling_strategy='majority')\n",
        "\n",
        "            pipeline = Pipeline([\n",
        "                ('over', over),\n",
        "                ('under', under)\n",
        "            ])\n",
        "\n",
        "            X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train_reconstructed, y_train)\n",
        "\n",
        "            print(\"Class distribution after resampling:\", np.bincount(y_train_resampled))\n",
        "\n",
        "            results, df = model_search(\n",
        "                models=models,\n",
        "                params=params,\n",
        "                over=over,\n",
        "                under=under,\n",
        "                X_train=X_train_resampled,\n",
        "                y_train=y_train_resampled,\n",
        "                X_test=X_test_reconstructed,\n",
        "                y_test=y_test,\n",
        "                X_val=X_val_reconstructed,\n",
        "                y_val=y_val\n",
        "            )\n",
        "\n",
        "            file_path = data_type + '_model_training_results.xlsx'\n",
        "            save_data_splits(X_train_resampled, X_test_reconstructed, X_val_reconstructed, y_train_resampled, y_test, y_val, os.path.join('covid_baseline_classifier_outputs', data_type))\n",
        "            df.to_excel(file_path)\n",
        "\n",
        "        elif data_type == 'Proteomics':\n",
        "            X_train_preprocessed, X_val_preprocessed, X_test_preprocessed = omics_pipeline(X_train, X_val, X_test, y_train, data_type)\n",
        "\n",
        "            X_train_reconstructed, X_val_reconstructed, X_test_reconstructed = vae_process(X_train_preprocessed, X_val_preprocessed, X_test_preprocessed, device, data_type)\n",
        "\n",
        "            print(\"Class distribution before resampling:\", np.bincount(y_train))\n",
        "            over = BorderlineSMOTE(sampling_strategy='auto')\n",
        "            under = RandomUnderSampler(sampling_strategy='majority')\n",
        "\n",
        "            pipeline = Pipeline([\n",
        "                ('over', over),\n",
        "                ('under', under)\n",
        "            ])\n",
        "\n",
        "            X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train_reconstructed, y_train)\n",
        "\n",
        "            print(\"Class distribution after resampling:\", np.bincount(y_train_resampled))\n",
        "\n",
        "            results, df = model_search(\n",
        "                models=models,\n",
        "                params=params,\n",
        "                over=over,\n",
        "                under=under,\n",
        "                X_train=X_train_resampled,\n",
        "                y_train=y_train_resampled,\n",
        "                X_test=X_test_reconstructed,\n",
        "                y_test=y_test,\n",
        "                X_val=X_val_reconstructed,\n",
        "                y_val=y_val\n",
        "            )\n",
        "\n",
        "            file_path = data_type + '_model_training_results.xlsx'\n",
        "            save_data_splits(X_train_resampled, X_test_reconstructed, X_val_reconstructed, y_train_resampled, y_test, y_val, os.path.join('covid_baseline_classifier_outputs', data_type))\n",
        "            df.to_excel(file_path)\n",
        "\n",
        "    return X_train_resampled, X_val_preprocessed, X_test_preprocessed, y_train_resampled, y_test, y_val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kfudR7fRW5l7",
      "metadata": {
        "id": "kfudR7fRW5l7"
      },
      "source": [
        "**Checking Summary Stats**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tAKT56hTW8Vn",
      "metadata": {
        "id": "tAKT56hTW8Vn"
      },
      "outputs": [],
      "source": [
        "def generate_feature_statistics(data_paths):\n",
        "    for file_path in data_paths:\n",
        "        dataset_name = file_path.split(\"/\")[-1].replace(\".xlsx\", \"\")\n",
        "\n",
        "        data = pd.read_excel(file_path)\n",
        "\n",
        "        feature_stats = data.describe().transpose()\n",
        "\n",
        "        output_file = f\"{dataset_name}_feature_statistics.csv\"\n",
        "        feature_stats.to_csv(output_file)\n",
        "\n",
        "        print(f\"Feature statistics for {dataset_name} dataset saved to {output_file}\")\n",
        "        print(feature_stats, \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0HCxr-PtYhTx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HCxr-PtYhTx",
        "outputId": "2437764e-71f5-43c1-fc8c-d1ad3810d3c8"
      },
      "outputs": [],
      "source": [
        "data_paths = [\n",
        "    \"/content/drive/MyDrive/Master's Thesis/Final Plan/COVID-19/final_data/isb_clinical.xlsx\",\n",
        "    \"/content/drive/MyDrive/Master's Thesis/Final Plan/COVID-19/final_data/isb_metabolomics.xlsx\",\n",
        "    \"/content/drive/MyDrive/Master's Thesis/Final Plan/COVID-19/final_data/isb_proteomics.xlsx\"\n",
        "]\n",
        "\n",
        "generate_feature_statistics(data_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q0Pv1bRR9mfR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q0Pv1bRR9mfR",
        "outputId": "795cb629-2595-4d94-d3e4-f4cd4c3a09fd"
      },
      "outputs": [],
      "source": [
        "main(data_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jLt9aJ0jIx2k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLt9aJ0jIx2k",
        "outputId": "25c6d746-6efa-4d04-e2f1-5925f052859e"
      },
      "outputs": [],
      "source": [
        "!zip -r covid_baseline_classifier_outputs.zip covid_baseline_classifier_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QZPRPgrOXquQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZPRPgrOXquQ",
        "outputId": "d1afa47c-17a6-41e8-c4f5-3807fc2720a4"
      },
      "outputs": [],
      "source": [
        "!zip -r vae_outputs.zip vae_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ba158d1",
      "metadata": {
        "id": "8ba158d1",
        "scrolled": true
      },
      "source": [
        "## Interpretability on Individual Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wFsWQDXsYXG4",
      "metadata": {
        "id": "wFsWQDXsYXG4"
      },
      "source": [
        "{'xgb__subsample': 1.0, 'xgb__reg_lambda': 0.2, 'xgb__reg_alpha': 0.8, 'xgb__n_estimators': 200, 'xgb__max_depth': 3, 'xgb__learning_rate': 0.25, 'xgb__gamma': 0.2, 'xgb__colsample_bytree': 0.8}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74d6b0fc",
      "metadata": {
        "id": "74d6b0fc"
      },
      "outputs": [],
      "source": [
        "proteomics_model = ExtraTreesClassifier(\n",
        "        n_estimators=500,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        max_depth=None,\n",
        "        criterion='gini',\n",
        "        class_weight=None\n",
        ")\n",
        "\n",
        "metabolomics_model = SVC(\n",
        "    probability=True,  \n",
        "    max_iter=700,      \n",
        "    kernel='poly',    \n",
        "    gamma='auto',      \n",
        "    class_weight=None, \n",
        "    C=100              \n",
        ")\n",
        "\n",
        "clinical_model = RandomForestClassifier(\n",
        "    n_estimators=50,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=4,\n",
        "    max_depth=20,\n",
        "    criterion='gini',\n",
        "    class_weight=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AxkCBploVE3n",
      "metadata": {
        "id": "AxkCBploVE3n"
      },
      "outputs": [],
      "source": [
        "def load_pre_split_data():\n",
        "        X_train_pro = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Proteomics/X_train.pkl')\n",
        "        y_train_pro = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Proteomics/y_train.pkl')\n",
        "        X_test_pro = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Proteomics/X_test.pkl')\n",
        "        y_test_pro = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Proteomics/y_test.pkl')\n",
        "        X_val_pro = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Proteomics/X_val.pkl')\n",
        "        y_val_pro = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Proteomics/y_val.pkl')\n",
        "\n",
        "        X_train_met = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Metabolomics/X_train.pkl')\n",
        "        y_train_met = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Metabolomics/y_train.pkl')\n",
        "        X_test_met = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Metabolomics/X_test.pkl')\n",
        "        y_test_met = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Metabolomics/y_test.pkl')\n",
        "        X_val_met = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Metabolomics/X_val.pkl')\n",
        "        y_val_met = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Metabolomics/y_val.pkl')\n",
        "\n",
        "        X_train_clin = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Clinical/X_train.pkl')\n",
        "        y_train_clin = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Clinical/y_train.pkl')\n",
        "        X_test_clin = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Clinical/X_test.pkl')\n",
        "        y_test_clin = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Clinical/y_test.pkl')\n",
        "        X_val_clin = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Clinical/X_val.pkl')\n",
        "        y_val_clin = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Clinical/y_val.pkl')\n",
        "\n",
        "        return X_train_met, y_train_met, X_test_met, y_test_met, X_val_met, y_val_met, X_train_pro, y_train_pro, X_test_pro, y_test_pro, X_val_pro, y_val_pro, X_train_clin, y_train_clin, X_test_clin, y_test_clin, X_val_clin, y_val_clin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SZ2uStaGid8O",
      "metadata": {
        "id": "SZ2uStaGid8O"
      },
      "outputs": [],
      "source": [
        "def load_model(data_type):\n",
        "  if data_type == \"Proteomics\":\n",
        "      model = proteomics_model\n",
        "\n",
        "      explainer_type = 'tree'\n",
        "      shap_filename = 'pro_shap.svg'\n",
        "      lime_filename = 'pro_lime.html'\n",
        "      cm_filename = 'pro_confusion_matrix.png'\n",
        "      output_file_name = 'pro_feature_importance.xlsx'\n",
        "      index = 10\n",
        "  elif data_type == \"Metabolomics\":\n",
        "      model = metabolomics_model\n",
        "\n",
        "      explainer_type = 'kernel'\n",
        "      shap_filename = 'met_shap.svg'\n",
        "      lime_filename = 'met_lime.html'\n",
        "      cm_filename = 'met_confusion_matrix.png'\n",
        "      output_file_name = 'met_feature_importance.xlsx'\n",
        "      index = 10\n",
        "  elif data_type == \"Clinical\":\n",
        "      model = clinical_model\n",
        "      loading_matrix = None\n",
        "      explainer_type = 'tree'\n",
        "      shap_filename = 'clin_shap.svg'\n",
        "      lime_filename = 'clin_lime.html'\n",
        "      cm_filename = 'clin_confusion_matrix.png'\n",
        "      output_file_name = 'clin_feature_importance.xlsx'\n",
        "      index = 9\n",
        "  else:\n",
        "      raise ValueError(f\"Unexpected data_type: {data_type}\")\n",
        "  return model, explainer_type, shap_filename, lime_filename, cm_filename, output_file_name, index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a668a15",
      "metadata": {
        "id": "1a668a15"
      },
      "outputs": [],
      "source": [
        "def train_and_explain_model(model, X_train, y_train, X_test, y_test, explainer_type, shap_filename, lime_filename, cm_filename, output_dir, output_filename, i):\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('True labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig(cm_filename, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    # handle multi-class ROC for multiclass classification\n",
        "    if len(np.unique(y_train)) > 2:\n",
        "        for class_idx in range(y_pred_proba.shape[1]):\n",
        "          # create binary labels for the current class\n",
        "          y_test_bin = (y_test == class_idx).astype(int)\n",
        "\n",
        "          # calculate ROC curve for this binary classification\n",
        "          fpr[class_idx], tpr[class_idx], _ = roc_curve(y_test_bin, y_pred_proba[:, class_idx])\n",
        "          roc_auc[class_idx] = auc(fpr[class_idx], tpr[class_idx])\n",
        "\n",
        "          # plot and save the ROC curve\n",
        "          plt.figure()\n",
        "          plt.plot(fpr[class_idx], tpr[class_idx], color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc[class_idx]:.2f})')\n",
        "          plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "          plt.xlim([0.0, 1.0])\n",
        "          plt.ylim([0.0, 1.05])\n",
        "          plt.xlabel('False Positive Rate')\n",
        "          plt.ylabel('True Positive Rate')\n",
        "          plt.title(f'Receiver Operating Characteristic - Class {class_idx}')\n",
        "          plt.legend(loc=\"lower right\")\n",
        "          roc_filename = f\"{output_dir}/roc_curve_class_{class_idx}.png\"\n",
        "          plt.savefig(roc_filename, dpi=300)\n",
        "          plt.close()\n",
        "\n",
        "    else:\n",
        "        # for binary classification\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # plot ROC curve\n",
        "        plt.figure()\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver Operating Characteristic')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        roc_filename = f\"{output_dir}/roc_curve.png\"\n",
        "        plt.savefig(roc_filename, dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    # SHAP Explainer\n",
        "    if explainer_type == \"tree\":\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "    elif explainer_type == \"kernel\":\n",
        "        explainer = shap.KernelExplainer(model.predict_proba, X_train)\n",
        "    elif explainer_type == \"linear\":\n",
        "        explainer = shap.LinearExplainer(model, X_train)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid explainer type\")\n",
        "\n",
        "    shap_values = explainer.shap_values(X_train)\n",
        "\n",
        "    # check if SHAP values are for a multiclass case\n",
        "    print(f\"SHAP values shape: {np.array(shap_values).shape}\")  \n",
        "\n",
        "    # if it's a multiclass problem (3D array), iterate over classes\n",
        "    if isinstance(shap_values, list) or len(shap_values.shape) == 3:\n",
        "        num_classes = shap_values.shape[2] if len(shap_values.shape) == 3 else len(shap_values)\n",
        "        for class_idx in range(num_classes):\n",
        "            # select SHAP values for the current class\n",
        "            shap_values_for_plot = shap_values[:, :, class_idx]\n",
        "\n",
        "            # ensure feature names are passed for the bar plot\n",
        "            feature_names = list(X_train.columns.values)\n",
        "\n",
        "            # generate SHAP summary plot (bar chart) for the current class\n",
        "            shap.summary_plot(shap_values_for_plot, X_train, feature_names=feature_names, plot_type='bar', show=False)\n",
        "\n",
        "            # save the SHAP plot for this class\n",
        "            class_shap_filename = f\"{shap_filename.split('.')[0]}_class_{class_idx}.svg\" \n",
        "            plt.savefig(class_shap_filename, dpi=300)\n",
        "            plt.close()\n",
        "    else:\n",
        "        shap_values_for_plot = shap_values\n",
        "\n",
        "        # generate SHAP summary plot (bar chart)\n",
        "        feature_names = list(X_train.columns.values)\n",
        "        shap.summary_plot(shap_values_for_plot, X_train, feature_names=feature_names, plot_type='bar', show=False)\n",
        "\n",
        "        # save SHAP plot\n",
        "        plt.savefig(shap_filename, dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    # LIME Explainer\n",
        "    explainer = lime_tabular.LimeTabularExplainer(X_train.to_numpy(),\n",
        "                                                  feature_names=X_train.columns,\n",
        "                                                  class_names=[\"Mild\", \"Moderate\", \"Severe\"],\n",
        "                                                  mode='classification',\n",
        "                                                  random_state=42)\n",
        "    exp = explainer.explain_instance(X_test.iloc[i].to_numpy(), model.predict_proba, num_features=10)\n",
        "    exp.save_to_file(os.path.join(os.getcwd(), lime_filename))\n",
        "\n",
        "    observation = X_test.iloc[i]\n",
        "    observation_df = pd.DataFrame(observation).transpose()\n",
        "    observation_df['who_severity'] = y_test.iloc[i]\n",
        "    print(observation_df)\n",
        "\n",
        "    return y_pred_proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "npEZB4R3VeEg",
      "metadata": {
        "id": "npEZB4R3VeEg"
      },
      "outputs": [],
      "source": [
        "data_paths = ['/content/X_clinical.pkl',\n",
        "              'covid_baseline_classifier_outputs/Metabolomics',\n",
        "              'covid_baseline_classifier_outputs/Proteomics'\n",
        "              ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lUjtSXVkBhMV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUjtSXVkBhMV",
        "outputId": "749eae49-574a-4cb6-e94c-5696ad5c2d4f"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade xgboost shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9V3qQET0pnWe",
      "metadata": {
        "id": "9V3qQET0pnWe"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train_met, y_train_met, X_test_met, y_test_met, X_val_met, y_val_met, X_train_pro, y_train_pro, X_test_pro, y_test_pro, X_val_pro, y_val_pro, X_train_clin, y_train_clin, X_test_clin, y_test_clin, X_val_clin, y_val_clin = load_pre_split_data()\n",
        "X_train_met.columns = X_train_met.columns.str.replace('[', '').str.replace(']', '').str.replace('<', '')\n",
        "X_test_met.columns = X_test_met.columns.str.replace('[', '').str.replace(']', '').str.replace('<', '')\n",
        "X_train_met.columns = X_train_met.columns.astype(str)\n",
        "X_test_met.columns = X_test_met.columns.astype(str)\n",
        "\n",
        "output_dir_met = \"output/metabolomics\"\n",
        "output_dir_pro = \"output/proteomics\"\n",
        "output_dir_clin = \"output/clinical\"\n",
        "output_dir_stack = \"output/stacking\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A_jGeYQlVonY",
      "metadata": {
        "id": "A_jGeYQlVonY"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "met_pred_proba_test = train_and_explain_model(\n",
        "  metabolomics_model, X_train_met, y_train_met, X_test_met, y_test_met,\n",
        "  'kernel', 'met_shap.svg', 'met_lime.html', 'met_confusion_matrix.png',\n",
        "  output_dir_met, 'met_feature_importance.xlsx', 10\n",
        ")\n",
        "\n",
        "pro_pred_proba_test = train_and_explain_model(\n",
        "  proteomics_model, X_train_pro, y_train_pro, X_test_pro, y_test_pro,\n",
        "  'tree', 'pro_shap.svg', 'pro_lime.html', 'pro_confusion_matrix.png',\n",
        "  output_dir_pro, 'pro_feature_importance.xlsx', 10\n",
        ")\n",
        "\n",
        "clin_pred_proba_test = train_and_explain_model(\n",
        "  clinical_model, X_train_clin, y_train_clin, X_test_clin, y_test_clin,\n",
        "  'kernel', 'clin_shap.svg', 'clin_lime.html', 'clin_confusion_matrix.png',\n",
        "  output_dir_clin, 'clin_feature_importance.xlsx', 10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K5Akri5RZR_M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Akri5RZR_M",
        "outputId": "9db82bdc-a86f-4c2a-805f-c2ee1f27d8ea"
      },
      "outputs": [],
      "source": [
        "!zip -r output.zip output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ivDZ6gFUGh8D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivDZ6gFUGh8D",
        "outputId": "f7d31f6b-c482-4993-dd3b-2aa4597a67dd"
      },
      "outputs": [],
      "source": [
        "shap_values = np.load(\"/content/output/metabolomics/met_feature_importance.xlsx_shap_values.npy\")\n",
        "\n",
        "feature_names = X_train_met.columns\n",
        "\n",
        "print(\"SHAP values shape:\", shap_values.shape)\n",
        "\n",
        "# loop over all classes (3 classes in this case)\n",
        "for class_idx in range(shap_values.shape[2]):\n",
        "    # mean absolute SHAP values for the current class\n",
        "    class_shap_values = np.abs(shap_values[:, :, class_idx]).mean(axis=0)\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': class_shap_values\n",
        "    })\n",
        "\n",
        "    # sort by importance\n",
        "    feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
        "\n",
        "    excel_filename = f\"met_feature_importance_class_{class_idx}.xlsx\"\n",
        "    feature_importance.to_excel(excel_filename, index=False)\n",
        "\n",
        "    print(f\"Feature Importance for Class {class_idx} saved to {excel_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n8AFYNacGmB2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8AFYNacGmB2",
        "outputId": "b0ed06a6-75cc-4cdc-e095-7bd3001d243a"
      },
      "outputs": [],
      "source": [
        "shap_values = np.load(\"/content/output/proteomics/pro_feature_importance.xlsx_shap_values.npy\")\n",
        "\n",
        "feature_names = X_train_pro.columns\n",
        "\n",
        "print(\"SHAP values shape:\", shap_values.shape)\n",
        "\n",
        "# loop over all classes (3 classes in this case)\n",
        "for class_idx in range(shap_values.shape[2]):\n",
        "    # mean absolute SHAP values for the current class\n",
        "    class_shap_values = np.abs(shap_values[:, :, class_idx]).mean(axis=0)\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': class_shap_values\n",
        "    })\n",
        "\n",
        "    # sort by importance\n",
        "    feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
        "\n",
        "    excel_filename = f\"pro_feature_importance_class_{class_idx}.xlsx\"\n",
        "    feature_importance.to_excel(excel_filename, index=False)\n",
        "\n",
        "    print(f\"Feature Importance for Class {class_idx} saved to {excel_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GPLxLCO2-LOX",
      "metadata": {
        "id": "GPLxLCO2-LOX"
      },
      "outputs": [],
      "source": [
        "shap_values = np.load(\"/content/output/clinical/clin_feature_importance.xlsx_shap_values.npy\")\n",
        "\n",
        "feature_names = X_train_clin.columns\n",
        "\n",
        "print(\"SHAP values shape:\", shap_values.shape)\n",
        "\n",
        "# loop over all classes (3 classes in this case)\n",
        "for class_idx in range(shap_values.shape[2]):\n",
        "    # mean absolute SHAP values for the current class\n",
        "    class_shap_values = np.abs(shap_values[:, :, class_idx]).mean(axis=0)\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': class_shap_values\n",
        "    })\n",
        "\n",
        "    # sort by importance\n",
        "    feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
        "\n",
        "    excel_filename = f\"clin_feature_importance_class_{class_idx}.xlsx\"\n",
        "    feature_importance.to_excel(excel_filename, index=False)\n",
        "\n",
        "    print(f\"Feature Importance for Class {class_idx} saved to {excel_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b3d3b1",
      "metadata": {
        "id": "90b3d3b1"
      },
      "source": [
        "## Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a06070e5",
      "metadata": {
        "id": "a06070e5"
      },
      "outputs": [],
      "source": [
        "class CustomModel(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, model, feature_indices):\n",
        "        self.model = model\n",
        "        self.feature_indices = feature_indices\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.model.fit(X[:, self.feature_indices], y)\n",
        "        self.classes_ = self.model.classes_  # Set the classes_ attribute\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.model.predict_proba(X[:, self.feature_indices])\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X[:, self.feature_indices])\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\"model\": self.model, \"feature_indices\": self.feature_indices}\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TX3d7On4SlP8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX3d7On4SlP8",
        "outputId": "61a2c796-984e-438d-c950-aa6eddc23312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metabolomics Shape: (79, 891)\n",
            "Proteomics Shape: (79, 454)\n",
            "Clinical Shape: (79, 10)\n"
          ]
        }
      ],
      "source": [
        "# before concatenation, ensure all datasets have data\n",
        "print(f\"Metabolomics Shape: {X_train_met.shape}\")\n",
        "print(f\"Proteomics Shape: {X_train_pro.shape}\")\n",
        "print(f\"Clinical Shape: {X_train_clin.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lcvKcFo6Sgqr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcvKcFo6Sgqr",
        "outputId": "e799d91f-d3cb-49f0-a837-3272c1b28660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train_clin: (79, 10)\n",
            "Shape of X_test_clin: (40, 10)\n",
            "Shape of X_val_clin: (79, 10)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Shape of X_train_clin: {X_train_clin.shape}\")\n",
        "print(f\"Shape of X_test_clin: {X_test_clin.shape}\")\n",
        "print(f\"Shape of X_val_clin: {X_val_clin.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0D4dy10-V1d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D4dy10-V1d4",
        "outputId": "2cfc7c40-fed3-43f3-d82c-9f6662caa628"
      },
      "outputs": [],
      "source": [
        "X_train = np.concatenate([X_train_met, X_train_pro, X_train_clin], axis=1)\n",
        "X_test = np.concatenate([X_test_met, X_test_pro, X_test_clin], axis=1)\n",
        "X_val = np.concatenate([X_val_met, X_val_pro, X_val_clin], axis=1)\n",
        "y_train = y_train_clin\n",
        "y_test = y_test_clin\n",
        "y_val = y_val_clin\n",
        "\n",
        "# feature indices for each modality (Metabolomics and Proteomics)\n",
        "\n",
        "metabolomics_feature_indices = list(range(X_train_met.shape[1]))\n",
        "proteomics_feature_indices = list(range(X_train_met.shape[1], X_train_met.shape[1] + X_train_pro.shape[1]))\n",
        "clinical_feature_indices = list(range(X_train_met.shape[1] + X_train_pro.shape[1], X_train_met.shape[1] + X_train_pro.shape[1] + X_train_clin.shape[1]))\n",
        "\n",
        "print(\"Metabolomics Feature Indices:\", metabolomics_feature_indices)\n",
        "print(\"Proteomics Feature Indices:\", proteomics_feature_indices)\n",
        "print(\"Clinical Feature Indices:\", clinical_feature_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "369ec519",
      "metadata": {
        "id": "369ec519",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "base_models = [\n",
        "    ('svc', CustomModel(metabolomics_model, metabolomics_feature_indices)),\n",
        "    ('ext', CustomModel(proteomics_model, proteomics_feature_indices)),\n",
        "    ('rf', CustomModel(clinical_model, clinical_feature_indices)),\n",
        "]\n",
        "\n",
        "stacked_models = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression( random_state=42))\n",
        "\n",
        "\n",
        "stacked_models.fit(X_train, y_train)\n",
        "\n",
        "y_pred = stacked_models.predict(X_test)\n",
        "y_pred_proba = stacked_models.predict_proba(X_test)\n",
        "y_pred_val = stacked_models.predict(X_val)\n",
        "y_pred_proba_val = stacked_models.predict_proba(X_val)\n",
        "\n",
        "score = accuracy_score(y_test, y_pred)\n",
        "score_val = accuracy_score(y_val, y_pred_val)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
        "roc_auc_val = roc_auc_score(y_val, y_pred_proba_val, multi_class='ovr')\n",
        "fscore_macro = f1_score(y_test, y_pred, average='macro')\n",
        "fscore_macro_val = f1_score(y_val, y_pred_val, average='macro')\n",
        "fscore_weighted = f1_score(y_test, y_pred, average='weighted')\n",
        "fscore_weighted_val = f1_score(y_val, y_pred_val, average='weighted')\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "precision_val = precision_score(y_val, y_pred_val, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "recall_val = recall_score(y_val, y_pred_val, average='macro')\n",
        "recall_severe = recall_score(y_test, y_pred, labels=[2], average='macro')\n",
        "recall_severe_val = recall_score(y_val, y_pred_val, labels=[2], average='macro')\n",
        "\n",
        "\n",
        "X_full = np.concatenate([X_train, X_test, X_val], axis=0)\n",
        "y_full = np.concatenate([y_train, y_test, y_val], axis=0)\n",
        "\n",
        "cvscore = cross_val_score(stacked_models, X_full, y_full, cv=StratifiedKFold(10), scoring='roc_auc_ovr').mean()\n",
        "\n",
        "estimators_model = pd.DataFrame()\n",
        "estimators_model['Models'] = ['Stacking Ensemble Model']\n",
        "estimators_model['ROC AUC'] = [roc_auc]\n",
        "estimators_model['ROC AUC Validation'] = [roc_auc_val]\n",
        "estimators_model['F1-score (macro)'] = [fscore_macro]\n",
        "estimators_model['F1-score (macro) Validation'] = [fscore_macro_val]\n",
        "estimators_model['F1-score (weighted)'] = [fscore_weighted]\n",
        "estimators_model['F1-score (weighted) Validation'] = [fscore_weighted_val]\n",
        "estimators_model['Precision'] = [precision]\n",
        "estimators_model['Precision Validation'] = [precision_val]\n",
        "estimators_model['Recall'] = [recall]\n",
        "estimators_model['Recall Validation'] = [recall_val]\n",
        "estimators_model['Recall Severe'] = [recall_severe]\n",
        "estimators_model['Recall Severe Validation'] = [recall_severe_val]\n",
        "estimators_model['Score'] = [score]\n",
        "estimators_model['Score Validation'] = [score_val]\n",
        "estimators_model['CV Score'] = [cvscore]\n",
        "#estimators_model['Training Time'] = [processing_time]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c13b3333",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c13b3333",
        "outputId": "b4f1cb93-d58c-4e96-ffb4-541baa8b22ef",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(estimators_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a4c11e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "6a4c11e2",
        "outputId": "41ad245a-8629-4465-e16c-6d76597acb26",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.savefig('confusion_matrix.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2mzlSiq9pPxF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433,
          "referenced_widgets": [
            "c0cea53ba31b40b39b5d032e711346ed",
            "b065249e8a51468bb5e49dee65b9efb0",
            "a62b833ee2f04fe9b4fac65364edbb63",
            "f3ac87ddfd6a471ca60d8361f7d3c741",
            "e9e5cbdb0ac8429c88f5c51aa0273993",
            "fd1d585f0822442c93eb60f8e9b63110",
            "9d14f540d8894398bdae3881a3ec2b2d",
            "657ee6adea1f41889fc834b56dfd848e",
            "30c8f69de2d54f8099e47e980fe16025",
            "6942a1e1ce31464494c3ccaf579d4245",
            "4215d5d24a3f488c96c82ac4ce308fa5"
          ]
        },
        "id": "2mzlSiq9pPxF",
        "outputId": "a2ce7041-93de-4927-fdd1-a840c9c251d6"
      },
      "outputs": [],
      "source": [
        "stack_pred_proba_test = train_and_explain_model(\n",
        "  stacked_models, X_train, y_train, X_test, y_test,\n",
        "  'kernel', 'stack_shap.svg', 'stack_lime.html', 'stack_confusion_matrix.png',\n",
        "  output_dir_stack, 'stack_feature_importance.xlsx', 10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jB-UeYr7HJeN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB-UeYr7HJeN",
        "outputId": "38424818-fe69-401d-c842-2102eb6b8bdb"
      },
      "outputs": [],
      "source": [
        "!zip -r output.zip output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2WtABds7ZgF-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "2WtABds7ZgF-",
        "outputId": "091bbc8f-41c1-4742-c339-d4c6bd4dc4b1"
      },
      "outputs": [],
      "source": [
        "for data_path in data_paths:\n",
        "    data_type = data_path.split('/')[1]\n",
        "    print(\"Data type:\", data_type)\n",
        "    output_dir = os.path.join('covid_baseline_classifier_outputs', data_type)\n",
        "    X_train, y_train, X_test, y_test, X_val, y_val = load_pre_split_data(data_type)\n",
        "    model, explainer_type, shap_filename, lime_filename, cm_filename, output_file_name, index = load_model(data_type)\n",
        "    train_and_explain_model(model, X_train, y_train, X_test, y_test, explainer_type, shap_filename, lime_filename, cm_filename, output_dir, output_file_name, index)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "30c8f69de2d54f8099e47e980fe16025": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4215d5d24a3f488c96c82ac4ce308fa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "657ee6adea1f41889fc834b56dfd848e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6942a1e1ce31464494c3ccaf579d4245": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d14f540d8894398bdae3881a3ec2b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a62b833ee2f04fe9b4fac65364edbb63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_657ee6adea1f41889fc834b56dfd848e",
            "max": 79,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30c8f69de2d54f8099e47e980fe16025",
            "value": 79
          }
        },
        "b065249e8a51468bb5e49dee65b9efb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd1d585f0822442c93eb60f8e9b63110",
            "placeholder": "​",
            "style": "IPY_MODEL_9d14f540d8894398bdae3881a3ec2b2d",
            "value": "100%"
          }
        },
        "c0cea53ba31b40b39b5d032e711346ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b065249e8a51468bb5e49dee65b9efb0",
              "IPY_MODEL_a62b833ee2f04fe9b4fac65364edbb63",
              "IPY_MODEL_f3ac87ddfd6a471ca60d8361f7d3c741"
            ],
            "layout": "IPY_MODEL_e9e5cbdb0ac8429c88f5c51aa0273993"
          }
        },
        "e9e5cbdb0ac8429c88f5c51aa0273993": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3ac87ddfd6a471ca60d8361f7d3c741": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6942a1e1ce31464494c3ccaf579d4245",
            "placeholder": "​",
            "style": "IPY_MODEL_4215d5d24a3f488c96c82ac4ce308fa5",
            "value": " 79/79 [43:31&lt;00:00, 31.83s/it]"
          }
        },
        "fd1d585f0822442c93eb60f8e9b63110": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
